# Sub-Phase 0.1.4 Audit: data_quality.R (Detailed)
**File:** `R/functions/data_quality.R`  
**Lines:** 415  
**Date Audited:** February 12, 2026  
**Status:** ⚠️ **95% Universal Pattern, 30% COHA-Specific Values**

---

## Executive Summary

**Assessment:** Functions implement universal quality metrics *patterns*, but have significant COHA-specific hardcoded values embedded in logic.

**Functions:** 3 (all quality metric utilities)

**Key Issue:** Hardcoded column names and value ranges scattered throughout

**Recommendation:** **Option A (Parameterize) - Recommended**
- Extract hardcoded values to configurable parameters
- Make functions universally reusable
- Estimated effort: 2-3 hours to refactor + test

**Alternative:** Move to domain module (simpler, less reusable)

---

## Function Inventory

### 1. `compute_quality_metrics()`

**Location:** Lines 88-188 (with docs)

**Purpose:** Calculate multiple quality indicators for a data frame

**Parameters:**
```r
compute_quality_metrics <- function(df,
                                    required_columns = c("mass", "year", 
                                                         "dispersed"),
                                    min_rows = 10,
                                    verbose = FALSE)
```

**Default hardcoded columns:** ❌
```r
required_columns = c("mass", "year", "dispersed")  # ← COHA hardcoded
```

**Hardcoded Schema Validation:** ❌ (Lines 130-140)
```r
# Check mass column
if ("mass" %in% names(df) && is.numeric(df$mass)) {  # ← Hardcoded "mass"
  schema_correct <- schema_correct + 1
}
# Check year column  
if ("year" %in% names(df) && is.numeric(df$year)) {  # ← Hardcoded "year"
  schema_correct <- schema_correct + 1
}
# Check dispersed column
if ("dispersed" %in% names(df) && is.character(df$dispersed)) {  # ← Hardcoded "dispersed"
  schema_correct <- schema_correct + 1
}

metrics$schema_match <- schema_correct / 3 * 100  # ← Hardcoded denominator (3 columns)
```

**Problem:** Function always checks same 3 columns, doesn't use `required_columns` parameter!

**Hardcoded Value Ranges:** ❌ (Lines 167-178)
```r
# Mass range
if ("mass" %in% names(df)) {
  mass_issues <- sum(df$mass < 0 | df$mass > 1000, na.rm = TRUE)  # ← 0-1000 gm (COHA)
  metrics$outliers_detected <- metrics$outliers_detected + mass_issues
  if (mass_issues > 0) {
    metrics$warnings <- c(metrics$warnings,
                         sprintf("%d mass values outside 0-1000 range", mass_issues))  # ← Hardcoded
  }
}

# Year range
if ("year" %in% names(df)) {
  year_issues <- sum(df$year < 1980 | df$year > 2027, na.rm = TRUE)  # ← COHA study period
  metrics$outliers_detected <- metrics$outliers_detected + year_issues
  if (year_issues > 0) {
    metrics$warnings <- c(metrics$warnings,
                         sprintf("%d year values outside 1980-2027 range", year_issues))  # ← Hardcoded
  }
}
```

### 2. `calculate_quality_score()`

**Location:** Lines 247-344 (with docs)

**Purpose:** Aggregate quality metrics into single 0-100 score

**Parameters:**
```r
calculate_quality_score <- function(metrics, 
                                    weights = list(
                                      completeness = 0.30,
                                      schema = 0.30,
                                      row_count = 0.20,
                                      outliers = 0.20
                                    ))
```

**Assessment:** ✅ **Completely Universal**
- Weights are parameterizable (sensible defaults)
- No hardcoded references to COHA
- Works with any metrics from compute_quality_metrics()
- Scoring logic is generic

**Recommendation:** Keep as-is, move to core/ unchanged

### 3. `generate_quality_report()`

**Location:** Lines 360-415 (with docs)

**Purpose:** Create human-readable quality report

**Parameters:**
```r
generate_quality_report <- function(metrics, quality_score, verbose = TRUE)
```

**Assessment:** ✅ **Completely Universal**
- Takes metrics as input (generic)
- Formatting is independent of data schema
- Score interpretation ranges are generic (90+: excellent, 75+: good, etc.)
- Works with any quality score

**Recommendation:** Keep as-is, move to core/ unchanged

---

## Hardcoded Values Summary

### Hardcoded Column Names

| Line | Column | Context | COHA-Specific? |
|------|--------|---------|-----------------|
| 85 (default) | "mass", "year", "dispersed", "origin" | Function default | ✅ |
| 131 | "mass" | Schema check | ✅ |
| 134 | "year" | Schema check | ✅ |
| 137 | "dispersed" | Schema check | ✅ |
| 140 | 3 (denominator) | Schema scoring | ✅ |
| 169 | "mass" | Outlier detection | ✅ |
| 175 | "year" | Outlier detection | ✅ |

### Hardcoded Value Ranges

| Line | Range | What It Checks | COHA-Specific? |
|------|-------|----------------|-----------------|
| 169 | 0-1000 | Mass in grams | ✅ (Hawks: 200-1000g) |
| 175 | 1980-2027 | Year of study | ✅ (COHA study period) |

### Function Defaults

| Parameter | Default | COHA-Specific? |
|-----------|---------|-----------------|
| required_columns | c("mass", "year", "dispersed", "origin") | ✅ |
| min_rows | 10 | ❌ Generic |

---

## Core Problem: Logic Doesn't Use Parameters

**Issue:** Function accepts `required_columns` parameter but **ignores it** in logic!

**Current Code:**
```r
compute_quality_metrics <- function(df,
                                    required_columns = c("mass", "year", 
                                                         "dispersed"),  # ← Parameter
                                    min_rows = 10,
                                    verbose = FALSE) {
  # ...
  
  # Uses hardcoded column names, never checks required_columns!
  if ("mass" %in% names(df) && is.numeric(df$mass)) {  # ← Ignores parameter
    schema_correct <- schema_correct + 1
  }
```

**Problem:** User can pass custom `required_columns`, but function validates hardcoded columns instead.

---

## Refactoring Options

### Option A: Parameterize (Make Universal) ✅ RECOMMENDED

**Effort:** 2-3 hours

**Result:** Universal function works for any data schema

```r
#' Compute Data Quality Metrics (Universal)
#'
#' @param df Data frame
#' @param required_columns Character vector. Columns that must exist and correct types.
#'   Default: c("mass", "year", "dispersed")
#' @param column_types List. Expected types for columns.
#'   Default: list(mass = "numeric", year = "numeric", dispersed = "character")
#' @param outlier_ranges List. Valid ranges for numeric columns.
#'   Default: list(mass = c(0, 1000), year = c(1980, 2027))
#' @param min_rows Integer. Minimum acceptable row count. Default: 10
#' @param verbose Logical. Print details. Default: FALSE
#'
#' @return List with quality metrics
#'
#' @export
compute_quality_metrics <- function(df,
                                    required_columns = c("mass", "year", "dispersed"),
                                    column_types = list(
                                      mass = "numeric",
                                      year = "numeric",
                                      dispersed = "character"
                                    ),
                                    outlier_ranges = list(
                                      mass = c(0, 1000),
                                      year = c(1980, 2027)
                                    ),
                                    min_rows = 10,
                                    verbose = FALSE) {
  
  # ... USE required_columns and column_types parameters ...
  
  # Schema validation (uses parameters!)
  for (col in required_columns) {
    expected_type <- column_types[[col]]
    if (col %in% names(df)) {
      if (expected_type == "numeric" && is.numeric(df[[col]])) {
        schema_correct <- schema_correct + 1
      } else if (expected_type == "character" && is.character(df[[col]])) {
        schema_correct <- schema_correct + 1
      }
    }
  }
  
  metrics$schema_match <- schema_correct / length(required_columns) * 100  # Use param!
  
  # Outlier detection (uses parameters!)
  for (col in names(outlier_ranges)) {
    if (col %in% names(df) && is.numeric(df[[col]])) {
      range <- outlier_ranges[[col]]
      issues <- sum(df[[col]] < range[1] | df[[col]] > range[2], na.rm = TRUE)
      metrics$outliers_detected <- metrics$outliers_detected + issues
      if (issues > 0) {
        metrics$warnings <- c(metrics$warnings,
          sprintf("%d '%s' values outside %d-%d range", 
                  issues, col, range[1], range[2]))  # Dynamic!
      }
    }
  }
}
```

**Usage Examples:**

```r
# COHA analysis
coha_metrics <- compute_quality_metrics(
  df,
  required_columns = c("mass", "year", "dispersed"),
  column_types = list(mass = "numeric", year = "numeric", dispersed = "character"),
  outlier_ranges = list(mass = c(0, 1000), year = c(1980, 2027)),
  min_rows = 10
)

# Migration study
migration_metrics <- compute_quality_metrics(
  df,
  required_columns = c("origin", "destination", "distance"),
  column_types = list(origin = "character", destination = "character", 
                     distance = "numeric"),
  outlier_ranges = list(distance = c(0, 3000)),  # Different ranges!
  min_rows = 5  # Different minimum
)

# Temperature model
temp_metrics <- compute_quality_metrics(
  df,
  required_columns = c("temperature", "location", "date"),
  column_types = list(temperature = "numeric", location = "character", date = "character"),
  outlier_ranges = list(temperature = c(-50, 50)),  # Celsius range
  min_rows = 100
)
```

**Advantages:**
- ✅ Universal function works for ANY schema
- ✅ Reusable across all analyses
- ✅ No code duplication
- ✅ Clear configuration mechanism

**Disadvantages:**
- More complex API (more parameters)
- Requires refactoring and testing

---

### Option B: Move to Domain Module

**Effort:** 30 minutes

**Result:** COHA-specific quality metrics in domain module

```r
# domain_modules/coha_dispersal/data_quality.R
source(here::here("core", "data_quality.R"))  # Universal version

#' COHA-Specific Quality Metrics
#'
#' Convenience function for COHA analysis with pre-configured settings
compute_coha_quality_metrics <- function(df, verbose = FALSE) {
  compute_quality_metrics(
    df,
    required_columns = c("mass", "year", "dispersed", "origin"),
    column_types = list(
      mass = "numeric",
      year = "numeric",
      dispersed = "character",
      origin = "character"
    ),
    outlier_ranges = list(
      mass = c(0, 1000),    # Grams
      year = c(1980, 2027)  # COHA study period
    ),
    min_rows = 10,
    verbose = verbose
  )
}
```

**Advantages:**
- ✅ Simple (just move, no refactoring)
- ✅ COHA config in domain module (where it belongs)
- ✅ Core stays universal

**Disadvantages:**
- ❌ Can't reuse for other analyses (each needs wrapper)
- ❌ Code duplication across domains
- ❌ Less elegant

---

### Option C: Create Both (Best of Both?)

**Effort:** 3-4 hours

**Result:** Universal function + domain wrappers

1. Create universal `compute_quality_metrics()` in core/ (parameterized)
2. Create COHA wrapper in domain/ that calls universal version

**Combines benefits:** Clean universal core + easy COHA access

---

## Recommendation

### Primary: **Option A (Parameterize)** ✅

**Reasoning:**
1. Quality metrics are genuinely universal (all analyses need them)
2. No fundamental reason for hardcoding
3. Parameterization is clean and maintainable
4. Enables reuse across all domains
5. Better long-term design

**Action Plan:**
1. Refactor to use parameters
2. Create universal version in core/
3. Write comprehensive tests
4. Document with examples for different domains
5. Create COHA-specific convenience wrapper (optional)

### Secondary: **Option B (Move to Domain)** as fallback

If parameterization seems too complex:
- Move current version to domain module
- Create universal stub later (Phase 2+)
- Simpler for Phase 1

---

## Implementation Plan (Option A)

### Step 1: Update function signature
```r
compute_quality_metrics <- function(df,
                                    required_columns = c("mass", "year", "dispersed"),
                                    column_types = NULL,
                                    outlier_ranges = NULL,
                                    min_rows = 10,
                                    verbose = FALSE)
```

### Step 2: Set smart defaults
```r
# If not provided, infer from required_columns
if (is.null(column_types)) {
  column_types <- list()
  for (col in required_columns) {
    # Guess type based on column name or default to numeric
    if (col %in% c("dispersed", "origin", "location", "name")) {
      column_types[[col]] <- "character"
    } else {
      column_types[[col]] <- "numeric"
    }
  }
}
```

### Step 3: Use parameters in validation
```r
# Convert to loop over required_columns
for (col in required_columns) {
  col_type <- column_types[[col]]
  if (col %in% names(df)) {
    # Check type
    if (col_type == "numeric" && is.numeric(df[[col]])) {
      schema_correct <- schema_correct + 1
    } else if (col_type == "character" && is.character(df[[col]])) {
      schema_correct <- schema_correct + 1
    }
  }
}
```

### Step 4: Use outlier_ranges parameter
```r
if (!is.null(outlier_ranges)) {
  for (col in names(outlier_ranges)) {
    if (col %in% names(df) && is.numeric(df[[col]])) {
      range <- outlier_ranges[[col]]
      issues <- sum(df[[col]] < range[1] | df[[col]] > range[2], na.rm = TRUE)
      # ... etc
    }
  }
}
```

### Step 5: Test thoroughly
- Test with COHA defaults
- Test with custom values
- Test with missing columns
- Test edge cases

---

## Backward Compatibility

Current default values should maintain backward compatibility:
```r
required_columns = c("mass", "year", "dispersed")  # ← Current defaults
column_types = [inferred or COHA defaults]
outlier_ranges = list(mass = c(0, 1000), year = c(1980, 2027))  # ← Current
min_rows = 10  # ← Current
```

Existing code calling `compute_quality_metrics(df)` still works!

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Breaking change** | Low | High | Use same defaults, thorough testing |
| **Missing edge cases** | Medium | Medium | Comprehensive unit tests |
| **Performance regression** | Low | Low | Profile before/after |
| **Over-parameterization** | Low | Low | Provide sensible defaults |

**Overall:** Low risk with proper testing

---

## Testing Plan

### Unit Tests Needed

```r
# Test 1: COHA defaults work
metrics <- compute_quality_metrics(coha_data)
assert(metrics$completeness >= 90)

# Test 2: Custom columns work
metrics <- compute_quality_metrics(
  migration_data,
  required_columns = c("origin", "destination", "distance"),
  column_types = list(...),
  outlier_ranges = list(...)
)
assert(all(c("origin", "destination") %in% names(metrics)))

# Test 3: Missing parameters handled
metrics <- compute_quality_metrics(
  data,
  required_columns = c("x", "y", "z")
  # column_types and outlier_ranges omitted - should infer
)
assert(!is.null(metrics))

# Test 4: Backward compatibility
old_call_result <- compute_quality_metrics(df)  # No extra params
new_call_result <- compute_quality_metrics(
  df,
  required_columns = c("mass", "year", "dispersed"),
  outlier_ranges = list(mass = c(0, 1000), year = c(1980, 2027))
)
assert(old_call_result$quality_score == new_call_result$quality_score)
```

---

## Integration with Other Functions

**Independent of:**
- `calculate_quality_score()` - Already universal, no changes needed
- `generate_quality_report()` - Already universal, no changes needed

**Benefits Entire Ecosystem:**
Once parameterized, works with:
- Any data loading function
- Any validation pipeline
- Any analysis domain

---

## Phase 1 Action

### For Phase 1.5 (if taking Option A):

**Task:** Parameterize data_quality.R

**Subtasks:**
1. Create `core/data_quality.R` with universal `compute_quality_metrics()`
2. Add `column_types` and `outlier_ranges` parameters
3. Refactor logic to use parameters
4. Create domain module wrapper for COHA
5. Write comprehensive tests

**Time:** 3-4 hours

### For Phase 1.5 (if taking Option B):

**Task:** Move data_quality.R to domain module

**Subtasks:**
1. Create `domain_modules/coha_dispersal/data_quality.R`
2. Move all current functions as-is
3. Update call sites
4. Test thoroughly

**Time:** 1-2 hours

---

## Summary

Data quality metrics are universal in concept but COHA-specific in implementation. 

**Recommendation:** Parameterize to make universal (Option A for long-term value)

**Effort:** 3-4 hours for parameterization + testing

**Payoff:** Reusable across all future analyses

**Alternative:** Move to domain module for simplicity (Option B, 1-2 hours)

**Decision:** Choose approach and proceed in Phase 1.5

---

**Status:** Sub-Phase 0.1.4 Complete  
**Recommendation:** Option A (Parameterize) for universal quality metrics  
**Next:** Continue with 0.1.6 (report.R) audit

